'''
!/usr/bin/env python
-*- coding: utf-8 -*-
@Time    : 2025/7/19 00:18
@Author  : JunYU
@File    : search
'''
import os
import re
import json
import sys
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass

import langchain
import molbloom
import paperqa
import paperscraper
from langchain_community.utilities import SerpAPIWrapper
from langchain.base_language import BaseLanguageModel
from langchain.tools import BaseTool
from langchain_community.embeddings import OpenAIEmbeddings
from pypdf.errors import PdfReadError

# ä¿®å¤ç›¸å¯¹å¯¼å…¥é—®é¢˜
try:
    from ..utils import is_multiple_smiles, split_smiles
except ImportError:
    # å¦‚æœç›¸å¯¹å¯¼å…¥å¤±è´¥ï¼Œå°è¯•ç»å¯¹å¯¼å…¥
    try:
        from app.core.utils import is_multiple_smiles, split_smiles
    except ImportError:
        # å¦‚æœç»å¯¹å¯¼å…¥ä¹Ÿå¤±è´¥ï¼Œæ·»åŠ è·¯å¾„å¹¶å¯¼å…¥
        current_dir = os.path.dirname(os.path.abspath(__file__))
        parent_dir = os.path.dirname(current_dir)
        if parent_dir not in sys.path:
            sys.path.insert(0, parent_dir)
        try:
            from utils import is_multiple_smiles, split_smiles
        except ImportError:
            # æœ€åçš„å›é€€æ–¹æ¡ˆï¼šå®šä¹‰ç®€å•çš„æ›¿ä»£å‡½æ•°
            def is_multiple_smiles(text):
                return "." in text if text else False
            
            def split_smiles(text):
                return text.split(".") if text else []


def paper_scraper(search: str, pdir: str = "query", semantic_scholar_api_key: str = None) -> dict:
    try:
        # Windowsç³»ç»Ÿä¸æ”¯æŒsignal.SIGALRMï¼Œä½¿ç”¨ç®€å•çš„è¶…æ—¶å¤„ç†
        import platform
        if platform.system() == "Windows":
            # Windowsç³»ç»Ÿä½¿ç”¨ç®€å•çš„æœç´¢ï¼Œä¸è®¾ç½®è¶…æ—¶
            try:
                result = paperscraper.search_papers(
                    search,
                    pdir=pdir,
                    semantic_scholar_api_key=semantic_scholar_api_key,
                )
                
                # ç¡®ä¿è¿”å›çš„æ˜¯å­—å…¸ç±»å‹
                if not isinstance(result, dict):
                    print(f"Warning: paper_scraper returned non-dict result: {type(result)}")
                    return {"error": f"Invalid result type: {type(result)}"}
                
                return result
            except Exception as e:
                print(f"Error in paperscraper.search_papers: {e}")
                return {"error": f"Search failed: {str(e)}"}
        else:
            # Unix/Linuxç³»ç»Ÿä½¿ç”¨ä¿¡å·è¶…æ—¶
            import signal
            
            def timeout_handler(signum, frame):
                raise TimeoutError("Search timeout")
            
            # è®¾ç½®30ç§’è¶…æ—¶
            signal.signal(signal.SIGALRM, timeout_handler)
            signal.alarm(30)
            
            try:
                result = paperscraper.search_papers(
                    search,
                    pdir=pdir,
                    semantic_scholar_api_key=semantic_scholar_api_key,
                )
                signal.alarm(0)  # å–æ¶ˆè¶…æ—¶
                
                # ç¡®ä¿è¿”å›çš„æ˜¯å­—å…¸ç±»å‹
                if not isinstance(result, dict):
                    print(f"Warning: paper_scraper returned non-dict result: {type(result)}")
                    return {"error": f"Invalid result type: {type(result)}"}
                
                return result
            except TimeoutError:
                signal.alarm(0)  # å–æ¶ˆè¶…æ—¶
                return {"timeout": "Search timed out after 30 seconds"}
    except KeyError:
        return {}
    except Exception as e:
        print(f"Error in paper_scraper: {e}")
        return {"error": f"Search failed: {str(e)}"}


def paper_search(llm, query, semantic_scholar_api_key=None):
    prompt = langchain.prompts.PromptTemplate(
        input_variables=["question"],
        template="""
        I would like to find scholarly papers to answer
        this question: {question}. Your response must be at
        most 10 words long.
        'A search query that would bring up papers that can answer
        this question would be: '""",
    )

    query_chain = langchain.chains.llm.LLMChain(llm=llm, prompt=prompt)
    if not os.path.isdir("./query"):  # todo: move to ckpt
        os.mkdir("query/")
    search = query_chain.run(query)
    print("\nSearch:", search)
    papers = paper_scraper(search, pdir=f"query/{re.sub(' ', '', search)}", semantic_scholar_api_key=semantic_scholar_api_key)
    return papers


def scholar2result_llm(llm, query, k=5, max_sources=2, openai_api_key=None, semantic_scholar_api_key=None):
    """Useful to answer questions that require
    technical knowledge. Ask a specific question."""
    try:
        papers = paper_search(llm, query, semantic_scholar_api_key=semantic_scholar_api_key)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯æˆ–è¶…æ—¶
        if len(papers) == 0:
            return "Not enough papers found"
        
        # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯ä¿¡æ¯
        for key, value in papers.items():
            if isinstance(value, dict) and ("error" in value or "timeout" in value):
                return f"Search failed: {value.get('error', value.get('timeout', 'Unknown error'))}"
        
        print(f"Found {len(papers)} papers to process")
        
    except Exception as e:
        print(f"Error in scholar2result_llm: {e}")
        return f"Search failed: {str(e)}"
    # æ£€æŸ¥æ˜¯å¦æ˜¯DeepSeekæ¨¡å‹ï¼Œå¦‚æœæ˜¯åˆ™ä½¿ç”¨ä¸åŒçš„APIåŸºç¡€URL
    if hasattr(llm, 'model_name') and (llm.model_name.startswith('deepseek-chat') or llm.model_name.startswith('deepseek-reasoner')):
        embeddings = OpenAIEmbeddings(
            openai_api_key=openai_api_key,
            openai_api_base="https://api.deepseek.com/v1"
        )
    else:
        embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)
    
    docs = paperqa.Docs(
        llm=llm,
        summary_llm=llm,
        embeddings=embeddings,
    )
    not_loaded = 0
    for path, data in papers.items():
        try:
            # æ·»åŠ è°ƒè¯•ä¿¡æ¯
            print(f"Processing paper: path={path}, data_type={type(data)}, data={data}")
            
            # æ£€æŸ¥dataçš„ç±»å‹ï¼Œå¦‚æœæ˜¯å­—ç¬¦ä¸²åˆ™ç›´æ¥ä½¿ç”¨ï¼Œå¦‚æœæ˜¯å­—å…¸åˆ™è·å–citation
            if isinstance(data, dict):
                citation = data.get("citation", str(data))
            elif isinstance(data, str):
                citation = data
            else:
                citation = str(data)
            
            docs.add(path, citation)
        except (ValueError, FileNotFoundError, PdfReadError) as e:
            print(f"Error processing paper {path}: {e}")
            not_loaded += 1
        except Exception as e:
            print(f"Unexpected error processing paper {path}: {e}")
            not_loaded += 1

    if not_loaded > 0:
        print(f"\nFound {len(papers.items())} papers but couldn't load {not_loaded}.")
    else:
        print(f"\nFound {len(papers.items())} papers and loaded all of them.")

    try:
        # ä½¿ç”¨çº¿ç¨‹æ¥é¿å…äº‹ä»¶å¾ªç¯å†²çª
        import threading
        import queue
        
        result_queue = queue.Queue()
        
        def run_query():
            try:
                answer = docs.query(query, k=k, max_sources=max_sources)
                result_queue.put(("success", answer.formatted_answer))
            except Exception as e:
                result_queue.put(("error", str(e)))
        
        # åœ¨æ–°çº¿ç¨‹ä¸­è¿è¡ŒæŸ¥è¯¢
        thread = threading.Thread(target=run_query)
        thread.start()
        thread.join(timeout=60)  # 60ç§’è¶…æ—¶
        
        if thread.is_alive():
            return "æ–‡çŒ®æœç´¢è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•ã€‚"
        
        if result_queue.empty():
            return "æ–‡çŒ®æœç´¢å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•ã€‚"
        
        status, result = result_queue.get()
        if status == "success":
            return result
        else:
            print(f"Error in docs query: {result}")
            return f"Error processing papers: {result}"
            
    except Exception as e:
        print(f"Error in docs query: {e}")
        return f"Error processing papers: {str(e)}"


class Scholar2ResultLLM(BaseTool):
    name: str = "LiteratureSearch"
    description: str = (
        "Useful to answer questions that require technical "
        "knowledge. Ask a specific question."
    )
    llm: BaseLanguageModel = None
    openai_api_key: str = None
    semantic_scholar_api_key: str = None


    def __init__(self, llm, openai_api_key, semantic_scholar_api_key):
        super().__init__()
        self.llm = llm
        # api keys
        self.openai_api_key = openai_api_key
        self.semantic_scholar_api_key = semantic_scholar_api_key

    def _run(self, query) -> str:
        try:
            return scholar2result_llm(
                self.llm,
                query,
                openai_api_key=self.openai_api_key,
                semantic_scholar_api_key=self.semantic_scholar_api_key
            )
        except Exception as e:
            print(f"Error in LiteratureSearch tool: {e}")
            return f"Literature search failed: {str(e)}"

    async def _arun(self, query) -> str:
        """Use the tool asynchronously."""
        try:
            return self._run(query)
        except Exception as e:
            print(f"Error in LiteratureSearch async tool: {e}")
            return f"Literature search failed: {str(e)}"


def web_search(keywords, search_engine="google"):
    try:
        # æ£€æŸ¥APIå¯†é’¥
        serp_api_key = os.getenv("SERP_API_KEY")
        if not serp_api_key:
            return "SERP_API_KEY not found. Please set the environment variable."
        
        # è®¾ç½®ä»£ç†
        proxies = {
            'http': os.environ.get('http_proxy'),
            'https': os.environ.get('https_proxy')
        }
        
        # åˆ›å»ºSerpAPIWrapperå®ä¾‹
        serp_wrapper = SerpAPIWrapper(
            serpapi_api_key=serp_api_key, 
            search_engine=search_engine
        )
        
        # å¦‚æœè®¾ç½®äº†ä»£ç†ï¼Œå°è¯•é…ç½®SerpAPIä½¿ç”¨ä»£ç†
        if proxies.get('http') or proxies.get('https'):
            try:
                # è®¾ç½®requestsçš„ä»£ç†ï¼ˆå¦‚æœSerpAPIå†…éƒ¨ä½¿ç”¨requestsï¼‰
                import requests
                session = requests.Session()
                session.proxies.update(proxies)
                # æ³¨æ„ï¼šSerpAPIå¯èƒ½ä¸æ”¯æŒç›´æ¥è®¾ç½®ä»£ç†ï¼Œè¿™é‡Œåªæ˜¯å°è¯•
            except Exception as e:
                print(f"Warning: Could not set proxy for SerpAPI: {e}")
        
        # æ‰§è¡Œæœç´¢
        result = serp_wrapper.run(keywords)
        
        # æ£€æŸ¥ç»“æœ
        if not result or result.strip() == "":
            return "No results found for the search query."
        
        return result
        
    except Exception as e:
        error_msg = str(e)
        if "API key" in error_msg.lower():
            return "Invalid or missing SERP_API_KEY. Please check your API key."
        elif "quota" in error_msg.lower() or "limit" in error_msg.lower():
            return "SerpAPI quota exceeded or rate limit reached."
        elif "network" in error_msg.lower() or "connection" in error_msg.lower():
            return "Network connection error. Please check your internet connection and proxy settings."
        else:
            return f"Search failed: {error_msg}"


def wikipedia_search(query):
    """ç»´åŸºç™¾ç§‘æœç´¢åŠŸèƒ½"""
    try:
        from langchain_community.utilities import WikipediaAPIWrapper
        import wikipedia
        import requests
        
        # è®¾ç½®ä»£ç†
        proxies = {
            'http': os.environ.get('http_proxy'),
            'https': os.environ.get('https_proxy')
        }
        
        # åˆ›å»ºç»´åŸºç™¾ç§‘APIåŒ…è£…å™¨
        wiki_wrapper = WikipediaAPIWrapper()
        
        # è®¾ç½®wikipediaåº“çš„ä»£ç†
        if proxies.get('http') or proxies.get('https'):
            # ç›´æ¥è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œè®©wikipediaåº“ä½¿ç”¨ä»£ç†
            original_http_proxy = os.environ.get('http_proxy')
            original_https_proxy = os.environ.get('https_proxy')
            
            # ç¡®ä¿ç¯å¢ƒå˜é‡å·²è®¾ç½®
            if not original_http_proxy:
                os.environ['http_proxy'] = proxies['http']
            if not original_https_proxy:
                os.environ['https_proxy'] = proxies['https']
        
        # æ‰§è¡Œæœç´¢
        result = wiki_wrapper.run(query)
        return result
    except Exception as e:
        return f"ç»´åŸºç™¾ç§‘æœç´¢å¤±è´¥: {str(e)}"


@dataclass
class SearchResult:
    """æœç´¢ç»“æœæ•°æ®ç»“æ„"""
    title: str
    content: str
    url: str = ""
    source: str = ""
    relevance_score: float = 0.0
    metadata: Dict = None

class WebSearchReranker:
    """WebSearchä¸“ç”¨çš„é‡æ’åºå™¨"""
    
    def __init__(self, use_local_reranker: bool = False):
        self.use_local_reranker = use_local_reranker
        
        # åˆå§‹åŒ–æœ¬åœ°é‡æ’åºå™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.use_local_reranker:
            self._init_local_reranker()
    
    def _init_local_reranker(self):
        """åˆå§‹åŒ–æœ¬åœ°é‡æ’åºå™¨"""
        try:
            from sentence_transformers import CrossEncoder
            self.local_reranker = CrossEncoder('BAAI/bge-reranker-v2-m3')
        except ImportError:
            print("Warning: sentence-transformers not installed, falling back to keyword-based reranking")
            self.use_local_reranker = False
    
    def rerank_search_results(self, query: str, search_results: List[SearchResult], top_k: int = 5) -> List[SearchResult]:
        """
        å¯¹æœç´¢ç»“æœè¿›è¡Œé‡æ’åº
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            search_results: æœç´¢ç»“æœåˆ—è¡¨
            top_k: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            é‡æ’åºåçš„ç»“æœåˆ—è¡¨
        """
        if not search_results:
            return []
        
        if self.use_local_reranker:
            return self._local_rerank(query, search_results, top_k)
        else:
            return self._keyword_rerank(query, search_results, top_k)
    
    def _local_rerank(self, query: str, search_results: List[SearchResult], top_k: int) -> List[SearchResult]:
        """ä½¿ç”¨æœ¬åœ°é‡æ’åºå™¨"""
        try:
            # å‡†å¤‡é‡æ’åºæ•°æ®
            pairs = []
            for result in search_results:
                # ç»„åˆæ ‡é¢˜å’Œå†…å®¹ä½œä¸ºæ–‡æ¡£
                document_text = f"{result.title} {result.content}"
                pairs.append([query, document_text])
            
            # æ‰§è¡Œé‡æ’åº
            scores = self.local_reranker.predict(pairs)
            
            # æ›´æ–°ç›¸å…³æ€§åˆ†æ•°
            for i, result in enumerate(search_results):
                result.relevance_score = float(scores[i])
            
            # æŒ‰ç›¸å…³æ€§åˆ†æ•°æ’åº
            search_results.sort(key=lambda x: x.relevance_score, reverse=True)
            
            return search_results[:top_k]
            
        except Exception as e:
            print(f"Local reranking failed: {e}")
            # å›é€€åˆ°å…³é”®è¯é‡æ’åº
            return self._keyword_rerank(query, search_results, top_k)
    
    def _keyword_rerank(self, query: str, search_results: List[SearchResult], top_k: int) -> List[SearchResult]:
        """åŸºäºå…³é”®è¯çš„é‡æ’åº"""
        query_words = set(query.lower().split())
        
        for result in search_results:
            # è®¡ç®—å…³é”®è¯åŒ¹é…åˆ†æ•°
            title_words = set(result.title.lower().split())
            content_words = set(result.content.lower().split())
            
            # æ ‡é¢˜åŒ¹é…æƒé‡æ›´é«˜
            title_overlap = len(query_words.intersection(title_words))
            content_overlap = len(query_words.intersection(content_words))
            
            # è®¡ç®—ç›¸å…³æ€§åˆ†æ•°
            title_score = title_overlap / max(len(query_words), 1) * 0.7
            content_score = content_overlap / max(len(query_words), 1) * 0.3
            
            # åŒ–å­¦ç›¸å…³å…³é”®è¯åŠ æƒ
            chemistry_keywords = ['chemical', 'molecule', 'compound', 'reaction', 'synthesis', 
                                'catalyst', 'polymer', 'organic', 'inorganic', 'biochemistry']
            chemistry_bonus = 0
            for keyword in chemistry_keywords:
                if keyword in result.title.lower() or keyword in result.content.lower():
                    chemistry_bonus += 0.1
            
            result.relevance_score = min(title_score + content_score + chemistry_bonus, 1.0)
        
        # æŒ‰ç›¸å…³æ€§åˆ†æ•°æ’åº
        search_results.sort(key=lambda x: x.relevance_score, reverse=True)
        
        return search_results[:top_k]

def parse_search_results(raw_result: str, source: str = "web") -> List[SearchResult]:
    """è§£ææœç´¢ç»“æœ"""
    results = []
    
    if source == "wikipedia":
        # ç»´åŸºç™¾ç§‘ç»“æœé€šå¸¸æ˜¯ä¸€ä¸ªé•¿æ–‡æœ¬
        if raw_result and "ç»´åŸºç™¾ç§‘æœç´¢å¤±è´¥" not in raw_result:
            # ç®€å•åˆ†æ®µå¤„ç†
            paragraphs = raw_result.split('\n\n')
            for i, paragraph in enumerate(paragraphs[:3]):  # å–å‰3æ®µ
                if paragraph.strip():
                    result = SearchResult(
                        title=f"Wikipedia Section {i+1}",
                        content=paragraph.strip(),
                        source="wikipedia",
                        relevance_score=0.8 - i * 0.1  # å‰é¢çš„æ®µè½æƒé‡æ›´é«˜
                    )
                    results.append(result)
    else:
        # ç½‘ç»œæœç´¢ç»“æœè§£æ
        # è¿™é‡Œéœ€è¦æ ¹æ®SerpAPIçš„è¿”å›æ ¼å¼è¿›è¡Œè§£æ
        # ç®€åŒ–å¤„ç†ï¼šå°†æ•´ä¸ªç»“æœä½œä¸ºä¸€ä¸ªæ–‡æ¡£
        if raw_result and "No results found" not in raw_result:
            # å°è¯•æå–æ ‡é¢˜å’Œå†…å®¹
            lines = raw_result.split('\n')
            current_title = ""
            current_content = ""
            
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                
                # ç®€å•çš„æ ‡é¢˜æ£€æµ‹ï¼ˆé€šå¸¸è¾ƒçŸ­ä¸”å¯èƒ½åŒ…å«ç‰¹æ®Šå­—ç¬¦ï¼‰
                if len(line) < 100 and ('http' in line or line.isupper() or line.endswith(':')):
                    if current_title and current_content:
                        result = SearchResult(
                            title=current_title,
                            content=current_content,
                            source="web",
                            relevance_score=0.5
                        )
                        results.append(result)
                    current_title = line
                    current_content = ""
                else:
                    current_content += line + " "
            
            # æ·»åŠ æœ€åä¸€ä¸ªç»“æœ
            if current_title and current_content:
                result = SearchResult(
                    title=current_title,
                    content=current_content,
                    source="web",
                    relevance_score=0.5
                )
                results.append(result)
            
            # å¦‚æœè§£æå¤±è´¥ï¼Œå°†æ•´ä¸ªç»“æœä½œä¸ºä¸€ä¸ªæ–‡æ¡£
            if not results:
                result = SearchResult(
                    title="Web Search Result",
                    content=raw_result,
                    source="web",
                    relevance_score=0.5
                )
                results.append(result)
    
    return results

class WebSearch(BaseTool):
    name: str = "WebSearch"
    description: str = (
        "Input a specific question, returns an answer from web search or Wikipedia. "
        "For general knowledge questions, Wikipedia will be used. For specific or recent information, web search will be used. "
        "Uses reranking to improve result relevance and quality. "
        "Do not mention any specific molecule names, but use more general features to formulate your questions."
    )
    
    # æ·»åŠ å­—æ®µå£°æ˜
    serp_api_key: Optional[str] = None
    use_reranker: bool = True
    use_local_reranker: bool = False
    max_results: int = 5
    reranker: Optional[object] = None

    def __init__(self, 
                 serp_api_key: str = None, 
                 use_reranker: bool = True,
                 use_local_reranker: bool = False,
                 max_results: int = 5):
        super().__init__()
        
        # ç›´æ¥è®¾ç½®å±æ€§
        self.serp_api_key = serp_api_key
        self.use_reranker = use_reranker
        self.use_local_reranker = use_local_reranker
        self.max_results = max_results
        
        # åˆå§‹åŒ–é‡æ’åºå™¨
        if self.use_reranker:
            self.reranker = WebSearchReranker(use_local_reranker=self.use_local_reranker)

    def _run(self, query: str) -> str:
        # åˆ¤æ–­æ˜¯å¦é€‚åˆä½¿ç”¨ç»´åŸºç™¾ç§‘
        wikipedia_keywords = [
            'what is', 'who is', 'definition', 'history', 'background', 
            'introduction', 'overview', 'concept', 'theory', 'method',
            'process', 'technique', 'principle', 'basics', 'fundamentals'
        ]
        
        query_lower = query.lower()
        use_wikipedia = any(keyword in query_lower for keyword in wikipedia_keywords)
        
        all_results = []
        
        # ç»´åŸºç™¾ç§‘æœç´¢
        if use_wikipedia:
            wiki_result = wikipedia_search(query)
            if "ç»´åŸºç™¾ç§‘æœç´¢å¤±è´¥" not in wiki_result:
                wiki_results = parse_search_results(wiki_result, "wikipedia")
                all_results.extend(wiki_results)
        
        # ç½‘ç»œæœç´¢
        if not self.serp_api_key:
            if not all_results:
                return "No SerpAPI key found. This tool may not be used without a SerpAPI key."
        else:
            web_result = web_search(query)
            if "No results found" not in web_result and "failed" not in web_result.lower():
                web_results = parse_search_results(web_result, "web")
                all_results.extend(web_results)
        
        # å¦‚æœæ²¡æœ‰ç»“æœ
        if not all_results:
            return "No relevant results found for the query."
        
        # ä½¿ç”¨é‡æ’åºå™¨æå‡ç»“æœè´¨é‡
        if self.use_reranker and hasattr(self, 'reranker'):
            try:
                reranked_results = self.reranker.rerank_search_results(
                    query, all_results, self.max_results
                )
            except Exception as e:
                print(f"Reranking failed: {e}")
                reranked_results = all_results[:self.max_results]
        else:
            reranked_results = all_results[:self.max_results]
        
        # æ ¼å¼åŒ–è¾“å‡º
        return self._format_results(reranked_results, query)
    
    def _format_results(self, results: List[SearchResult], query: str) -> str:
        """æ ¼å¼åŒ–æœç´¢ç»“æœ"""
        if not results:
            return "No relevant results found."
        
        formatted_parts = []
        
        # æ·»åŠ æœç´¢æ‘˜è¦
        formatted_parts.append(f"ğŸ” æœç´¢æŸ¥è¯¢: {query}")
        formatted_parts.append(f"ğŸ“Š æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³ç»“æœ (å·²é‡æ’åº)")
        formatted_parts.append("")
        
        for i, result in enumerate(results, 1):
            part = f"ã€ç»“æœ {i}ã€‘"
            if result.source:
                part += f" ({result.source.upper()})"
            if hasattr(result, 'relevance_score') and result.relevance_score > 0:
                part += f" [ç›¸å…³æ€§: {result.relevance_score:.2f}]"
            
            part += f"\næ ‡é¢˜: {result.title}"
            part += f"\nå†…å®¹: {result.content[:500]}..."  # é™åˆ¶å†…å®¹é•¿åº¦
            
            if result.url:
                part += f"\né“¾æ¥: {result.url}"
            
            formatted_parts.append(part)
            formatted_parts.append("")
        
        return "\n".join(formatted_parts)

    async def _arun(self, query: str) -> str:
        """Use the tool asynchronously."""
        return self._run(query)


class PatentCheck(BaseTool):
    name: str = "PatentCheck"
    description: str = "Input SMILES, returns if molecule is patented. You may also input several SMILES, separated by a period."

    def _run(self, smiles: str) -> str:
        """Checks if compound is patented. Give this tool only one SMILES string"""
        if is_multiple_smiles(smiles):
            smiles_list = split_smiles(smiles)
        else:
            smiles_list = [smiles]
        try:
            output_dict = {}
            for smi in smiles_list:
                r = molbloom.buy(smi, canonicalize=True, catalog="surechembl")
                if r:
                    output_dict[smi] = "Patented"
                else:
                    output_dict[smi] = "Novel"
            return str(output_dict)
        except:
            return "Invalid SMILES string"

    async def _arun(self, query: str) -> str:
        """Use the tool asynchronously."""
        return self._run(query)
